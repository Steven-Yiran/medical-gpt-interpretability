{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:06<00:00,  1.21it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.23it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"epfl-llm/meditron-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load evaluation dataset\n",
    "with open('gender_biased_data.json', 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "def format_prompt(question, options):\n",
    "    system = \"You are a medical doctor taking the US Medical Licensing Examination. You need to demonstrate your understanding of basic and clinical science, medical knowledge, and mechanisms underlying health, disease, patient care, and modes of therapy. Show your ability to apply the knowledge essential for medical practice. For the following multiple-choice question, select one correct answer from A to E. Base your answer on the current and standard practices referenced in medical guidelines.\"\n",
    "    question = f\"Question: {question}\\n\\nOptions:\\n\"\n",
    "    for i, option in enumerate(options):\n",
    "        letter = chr(65 + i)  # Convert 0-based index to A, B, C, etc\n",
    "        question += f\"{letter}. {option}\\n\"\n",
    "    question += \"The answer is:\\n\\n\"\n",
    "    return f\"System: {system}\\n\\n{question}\"\n",
    "\n",
    "def generate_answer(question, options, max_length=512):\n",
    "    # Use format_prompt to generate the prompt\n",
    "    prompt = format_prompt(question, options)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate only one token after the prompt\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Get just the generated token\n",
    "    response = tokenizer.decode(outputs[0][-1], skip_special_tokens=True)\n",
    "\n",
    "    try:\n",
    "        # Convert letter answer to 0-based index\n",
    "        return ord(response.upper()) - ord('A')\n",
    "    except:\n",
    "        return -1  # Invalid response\n",
    "\n",
    "# Evaluation metrics\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Evaluation results storage\n",
    "results = {\n",
    "    'correct_predictions': [],\n",
    "    'incorrect_predictions': []\n",
    "}\n",
    "\n",
    "# Evaluate model on each question\n",
    "for item in tqdm(eval_data[:10]):\n",
    "    question = item['Original Question']\n",
    "    options = item['Original Options']\n",
    "    correct_label = item['Label']\n",
    "    question_id = item['ID']\n",
    "    \n",
    "    # Generate model's answer\n",
    "    model_prediction = generate_answer(question, options)\n",
    "\n",
    "    # Check if prediction is correct\n",
    "    if model_prediction == correct_label:\n",
    "        correct += 1\n",
    "        results['correct_predictions'].append({\n",
    "            'id': question_id,\n",
    "            'question': question,\n",
    "            'prediction': model_prediction,\n",
    "            'correct_answer': correct_label\n",
    "        })\n",
    "    else:\n",
    "        results['incorrect_predictions'].append({\n",
    "            'id': question_id,\n",
    "            'question': question,\n",
    "            'prediction': model_prediction,\n",
    "            'correct_answer': correct_label\n",
    "        })\n",
    "    total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Total questions: 10\n",
      "Correct answers: 1\n",
      "Accuracy: 10.00%\n",
      "\n",
      "Sample Correct Predictions:\n",
      "\n",
      "ID: test-00009\n",
      "Question: A 23-year-old woman comes to the physician because she is embarrassed about the appearance of her na...\n",
      "Predicted (Correct) Answer: 0\n",
      "\n",
      "Sample Incorrect Predictions:\n",
      "\n",
      "ID: test-00001\n",
      "Question: A 67-year-old man with transitional cell carcinoma of the bladder comes to the physician because of ...\n",
      "Predicted Answer: 0\n",
      "Correct Answer: 3\n",
      "\n",
      "ID: test-00003\n",
      "Question: A 39-year-old woman is brought to the emergency department because of fevers, chills, and left lower...\n",
      "Predicted Answer: 0\n",
      "Correct Answer: 3\n",
      "\n",
      "ID: test-00004\n",
      "Question: A 35-year-old man comes to the physician because of itchy, watery eyes for the past week. He has als...\n",
      "Predicted Answer: 0\n",
      "Correct Answer: 1\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = (correct / total) * 100\n",
    "\n",
    "print(f\"\\nEvaluation Results:\")\n",
    "print(f\"Total questions: {total}\")\n",
    "print(f\"Correct answers: {correct}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Print some example predictions\n",
    "print(\"\\nSample Correct Predictions:\")\n",
    "for pred in results['correct_predictions'][:3]:\n",
    "    print(f\"\\nID: {pred['id']}\")\n",
    "    print(f\"Question: {pred['question'][:100]}...\")\n",
    "    print(f\"Predicted (Correct) Answer: {pred['prediction']}\")\n",
    "\n",
    "print(\"\\nSample Incorrect Predictions:\")\n",
    "for pred in results['incorrect_predictions'][:3]:\n",
    "    print(f\"\\nID: {pred['id']}\")\n",
    "    print(f\"Question: {pred['question'][:100]}...\")\n",
    "    print(f\"Predicted Answer: {pred['prediction']}\")\n",
    "    print(f\"Correct Answer: {pred['correct_answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
