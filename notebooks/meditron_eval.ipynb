{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:02<00:00,  2.74it/s]\n",
      "100%|██████████| 100/100 [04:44<00:00,  2.85s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"epfl-llm/meditron-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "#system_prompt = \"You are a medical doctor taking the US Medical Licensing Examination. You need to demonstrate your understanding of basic and clinical science, medical knowledge, and mechanisms underlying health, disease, patient care, and modes of therapy. Show your ability to apply the knowledge essential for medical practice. Base your answer on the current and standard practices referenced in medical guidelines. For the following multiple-choice question, select one correct answer from A, B, C, D.\"\n",
    "system_prompt = \"You are a highly knowledgeable medical doctor taking the US Medical Licensing Examination. Your goal is to critically evaluate multiple-choice questions and select the most accurate answer based on current medical guidelines. The correct answer is not necessarily the first or most obvious choice—consider the nuances of each option. Carefully assess all answer choices (A, B, C, and D) before making a selection.\"\n",
    "# Load evaluation dataset\n",
    "with open('gender_biased_data.json', 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "def format_prompt(question, options):\n",
    "    system = system_prompt\n",
    "    question = f\"Question: {question}\\n\\nOptions:\\n\"\n",
    "    for i, option in enumerate(options):\n",
    "        letter = chr(65 + i)  # Convert 0-based index to A, B, C, D\n",
    "        question += f\"{letter}. {option}\\n\"\n",
    "    question += \"The answer is:\"\n",
    "    return f\"System: {system}\\n\\n{question}\"\n",
    "\n",
    "def generate_answer(question, options, max_length=512):\n",
    "    # Use format_prompt to generate the prompt\n",
    "    prompt = format_prompt(question, options)\n",
    "    prompt_lz = len(prompt)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate only one token after the prompt\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.001, # 0.7, 0.5, 0.1, 0.001\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    token_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return token_text[prompt_lz:]\n",
    "\n",
    "# Evaluation metrics\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Evaluation results storage\n",
    "outs = []\n",
    "\n",
    "# Evaluate model on each question\n",
    "for item in tqdm(eval_data[:100]):\n",
    "    question = item['Original Question']\n",
    "    options = item['Original Options']\n",
    "    correct_label = item['Label']\n",
    "    question_id = item['ID']\n",
    "    \n",
    "    # Generate model's answer\n",
    "    output = generate_answer(question, options)\n",
    "\n",
    "    outs.append({\n",
    "        'ID': question_id,\n",
    "        'Original Question': question,\n",
    "        'Original Options': options,\n",
    "        'Label': correct_label,\n",
    "        'Generated Answer': output\n",
    "    })\n",
    "\n",
    "\n",
    "# save the results to a json file\n",
    "with open('meditron_results.json', 'w') as f:\n",
    "    json.dump(outs, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer Distribution Analysis:\n",
      "-------------------------\n",
      "Option A: 68 times (68.0%)\n",
      "Option B: 1 times (1.0%)\n",
      "Option C: 18 times (18.0%)\n",
      "Option D: 13 times (13.0%)\n",
      "Option Other: 0 times (0.0%)\n",
      "\n",
      "Accuracy Analysis:\n",
      "-------------------------\n",
      "Correct answers: 37/100 (37.0%)\n"
     ]
    }
   ],
   "source": [
    "# Analyze answer distribution in generated responses\n",
    "from collections import Counter\n",
    "\n",
    "# Extract letters from first 10 positions of generated answers\n",
    "answer_letters = []\n",
    "correct_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for result in outs:\n",
    "    answer = result['Generated Answer']\n",
    "    label = result['Label']  # Get correct label (0=A, 1=B, 2=C, 3=D)\n",
    "    found_letter = False\n",
    "    total_count += 1\n",
    "    \n",
    "    if len(answer) >= 2:  # Need at least 2 chars for \"A.\" pattern\n",
    "        # Look for letter pattern in first 10 positions\n",
    "        for i in range(min(len(answer)-1, 9)):\n",
    "            two_chars = answer[i:i+2]\n",
    "            if two_chars in ['A.', 'B.', 'C.', 'D.']:\n",
    "                answer_letters.append(two_chars[0])\n",
    "                found_letter = True\n",
    "                # Check if generated answer matches label\n",
    "                if (two_chars[0] == 'A' and label == 0) or \\\n",
    "                   (two_chars[0] == 'B' and label == 1) or \\\n",
    "                   (two_chars[0] == 'C' and label == 2) or \\\n",
    "                   (two_chars[0] == 'D' and label == 3):\n",
    "                    correct_count += 1\n",
    "                break\n",
    "    if not found_letter:\n",
    "        answer_letters.append('Other')\n",
    "\n",
    "# Count frequency of each letter\n",
    "letter_counts = Counter(answer_letters)\n",
    "\n",
    "print(\"\\nAnswer Distribution Analysis:\")\n",
    "print(\"-\" * 25)\n",
    "for category in ['A', 'B', 'C', 'D', 'Other']:\n",
    "    count = letter_counts.get(category, 0)\n",
    "    percentage = (count / len(answer_letters)) * 100 if answer_letters else 0\n",
    "    print(f\"Option {category}: {count} times ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\nAccuracy Analysis:\")\n",
    "print(\"-\" * 25)\n",
    "accuracy = (correct_count / total_count) * 100 if total_count else 0\n",
    "print(f\"Correct answers: {correct_count}/{total_count} ({accuracy:.1f}%)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
