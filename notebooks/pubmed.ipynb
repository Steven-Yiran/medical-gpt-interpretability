{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/dev/medical-gpt-interpretability/ENV/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dsets import ClinicalAgeGroupDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/BioGPT-Large-PubMedQA\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BioGPT-Large-PubMedQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 93 samples\n"
     ]
    }
   ],
   "source": [
    "dataset = ClinicalAgeGroupDataset('../data')\n",
    "opposite_age_group = {\n",
    "    'infant': 'elderly',\n",
    "    'children': 'adults',\n",
    "    'adults': 'children',\n",
    "    'elderly': 'infant'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0]\n",
    "age_replacement = opposite_age_group[data['age_group']]\n",
    "corrupted_question = data['question'].replace(data['age_group'], age_replacement)\n",
    "corrupted_context = data['context'].replace(data['age_group'], age_replacement)\n",
    "\n",
    "original_prompt = f\"Question: {data['question']} Context: {data['context']} the answer to the question given the context is\"\n",
    "corrupted_prompt = f\"Question: {corrupted_question} Context: {corrupted_context} the answer to the question given the context is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_prob(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # Forward pass to get the logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "\n",
    "    # Extract logits at the last position (where answer starts)\n",
    "    logits = outputs.logits[0, -1, :]  # Shape: [vocab_size]\n",
    "\n",
    "    # Get token IDs for \"Yes\", \"No\", and \"Maybe\"\n",
    "    yes_token_id = tokenizer(\" Yes\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "    no_token_id = tokenizer(\" No\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "    maybe_token_id = tokenizer(\" Maybe\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "\n",
    "    # Extract logits for the specific tokens\n",
    "    answer_logits = logits[[yes_token_id, no_token_id, maybe_token_id]]\n",
    "    answer_labels = [\"Yes\", \"No\", \"Maybe\"]\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    answer_probs = F.softmax(answer_logits, dim=0)\n",
    "\n",
    "    return dict(zip(answer_labels, answer_probs.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Yes': 0.9996758699417114,\n",
       "  'No': 0.00032262789318338037,\n",
       "  'Maybe': 1.5754862943140324e-06},\n",
       " {'Yes': 0.9999408721923828,\n",
       "  'No': 5.877300282008946e-05,\n",
       "  'Maybe': 3.77662559003511e-07})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer_prob(original_prompt), get_answer_prob(corrupted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
