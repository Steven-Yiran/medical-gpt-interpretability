{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading BioGPT into TransformerLens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/research-1/medical-gpt-interpretability/ENV/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import (\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x76a7399c19c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BioGptConfig {\n",
       "  \"_name_or_path\": \"microsoft/biogpt\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"architectures\": [\n",
       "    \"BioGptForCausalLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 1024,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 4096,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"layerdrop\": 0.0,\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"model_type\": \"biogpt\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"scale_embedding\": true,\n",
       "  \"transformers_version\": \"4.45.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 42384\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH = \"microsoft/biogpt\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
    "\n",
    "config = hf_model.config\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioGptForCausalLM(\n",
      "  (biogpt): BioGptModel(\n",
      "    (embed_tokens): BioGptScaledWordEmbedding(42384, 1024, padding_idx=1)\n",
      "    (embed_positions): BioGptLearnedPositionalEmbedding(1026, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x BioGptDecoderLayer(\n",
      "        (self_attn): BioGptSdpaAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_fn): GELUActivation()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (output_projection): Linear(in_features=1024, out_features=42384, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_path = \"BioGPT_state_dict.pth\"\n",
    "torch.save(hf_model.state_dict(), state_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load into TransformerLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_dim = config.hidden_size // config.num_attention_heads\n",
    "\n",
    "hooked_config = HookedTransformerConfig(\n",
    "    n_layers=24,\n",
    "    d_model=1024,\n",
    "    d_head=64,\n",
    "    n_heads=16,\n",
    "    d_mlp=4096,\n",
    "    d_vocab=42384,\n",
    "    n_ctx=1024,\n",
    "    act_fn='gelu',\n",
    "    normalization_type=\"LN\"\n",
    ")\n",
    "model = HookedTransformer(hooked_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biogpt_to_transformer_lens_format(in_sd, n_layers, n_heads):\n",
    "    out_sd = {}\n",
    "    out_sd[\"pos_embed.W_pos\"] = in_sd[f\"biogpt.embed_positions.weight\"]\n",
    "    out_sd[\"embed.W_E\"] = in_sd[f\"biogpt.embed_tokens.weight\"]\n",
    "\n",
    "    out_sd[\"ln_final.w\"] = in_sd[f\"biogpt.layer_norm.weight\"]\n",
    "    out_sd[\"ln_final.b\"] = in_sd[f\"biogpt.layer_norm.bias\"]\n",
    "    out_sd[\"unembed.W_U\"] = in_sd[f\"output_projection.weight\"].T\n",
    "\n",
    "    for layer in range(n_layers):\n",
    "        out_sd[f\"blocks.{layer}.ln1.w\"] = in_sd[f\"biogpt.layers.{layer}.fc1.weight\"]\n",
    "        out_sd[f\"blocks.{layer}.ln1.b\"] = in_sd[f\"biogpt.layers.{layer}.fc1.bias\"]\n",
    "        out_sd[f\"blocks.{layer}.ln2.w\"] = in_sd[f\"biogpt.layers.{layer}.fc2.weight\"]\n",
    "        out_sd[f\"blocks.{layer}.ln2.b\"] = in_sd[f\"biogpt.layers.{layer}.fc2.bias\"]\n",
    "\n",
    "\n",
    "        out_sd[f\"blocks.{layer}.attn.W_Q\"] = einops.rearrange(\n",
    "            in_sd[f\"biogpt.layers.{layer}.self_attn.q_proj.weight\"],\n",
    "            \"(n_heads d_head) d_model -> n_heads d_model d_head\",\n",
    "            n_heads=n_heads,\n",
    "        )\n",
    "        out_sd[f\"blocks.{layer}.attn.b_Q\"] = einops.rearrange(\n",
    "            in_sd[f\"biogpt.layers.{layer}.self_attn.q_proj.bias\"],\n",
    "            \"(n_heads d_head) -> n_heads d_head\",\n",
    "            n_heads=n_heads,\n",
    "        )\n",
    "        out_sd[f\"blocks.{layer}.attn.W_K\"] = einops.rearrange(\n",
    "            in_sd[f\"biogpt.layers.{layer}.self_attn.k_proj.weight\"],\n",
    "            \"(n_heads d_head) d_model -> n_heads d_model d_head\",\n",
    "            n_heads=n_heads,\n",
    "        )\n",
    "        out_sd[f\"blocks.{layer}.attn.b_K\"] = einops.rearrange(\n",
    "            in_sd[f\"biogpt.layers.{layer}.self_attn.k_proj.bias\"],\n",
    "            \"(n_heads d_head) -> n_heads d_head\",\n",
    "            n_heads=n_heads,\n",
    "        )\n",
    "        out_sd[f\"blocks.{layer}.attn.W_V\"] = einops.rearrange(\n",
    "            in_sd[f\"biogpt.layers.{layer}.self_attn.v_proj.weight\"],\n",
    "            \"(n_heads d_head) d_model -> n_heads d_model d_head\",\n",
    "            n_heads=n_heads,\n",
    "        )\n",
    "        out_sd[f\"blocks.{layer}.attn.b_V\"] = einops.rearrange(\n",
    "            in_sd[f\"biogpt.layers.{layer}.self_attn.v_proj.bias\"],\n",
    "            \"(n_heads d_head) -> n_heads d_head\",\n",
    "            n_heads=n_heads,\n",
    "        )\n",
    "        out_sd[f\"blocks.{layer}.attn.W_O\"] = einops.rearrange(\n",
    "            in_sd[f\"biogpt.layers.{layer}.self_attn.out_proj.weight\"],\n",
    "            \"(d_model n_heads) d_head -> n_heads d_model d_head\",\n",
    "            n_heads=n_heads,\n",
    "        )\n",
    "        out_sd[f\"blocks.{layer}.attn.b_O\"] = einops.rearrange(\n",
    "            in_sd[f\"biogpt.layers.{layer}.self_attn.out_proj.bias\"],\n",
    "            \"(d_model n_heads) -> n_heads d_model\",\n",
    "            n_heads=n_heads,\n",
    "        )\n",
    "\n",
    "        out_sd[f\"blocks.{layer}.mlp.b_in\"] = in_sd[f\"biogpt.layers.{layer}.fc1.bias\"]\n",
    "        out_sd[f\"blocks.{layer}.mlp.W_in\"] = in_sd[f\"biogpt.layers.{layer}.fc1.weight\"].T\n",
    "        out_sd[f\"blocks.{layer}.mlp.b_out\"] = in_sd[f\"biogpt.layers.{layer}.fc2.bias\"]\n",
    "        out_sd[f\"blocks.{layer}.mlp.W_out\"] = in_sd[f\"biogpt.layers.{layer}.fc2.weight\"].T\n",
    "\n",
    "    return out_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(state_dict_path, weights_only=False)\n",
    "\n",
    "tl_dict = biogpt_to_transformer_lens_format(state_dict, config.num_hidden_layers, config.num_attention_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.ln1.w torch.Size([4096, 1024])\n",
      "blocks.0.ln1.b torch.Size([4096])\n",
      "blocks.0.ln2.w torch.Size([1024, 4096])\n",
      "blocks.0.ln2.b torch.Size([1024])\n",
      "blocks.0.attn.W_Q torch.Size([16, 1024, 64])\n",
      "blocks.0.attn.b_Q torch.Size([16, 64])\n",
      "blocks.0.attn.W_K torch.Size([16, 1024, 64])\n",
      "blocks.0.attn.b_K torch.Size([16, 64])\n",
      "blocks.0.attn.W_V torch.Size([16, 1024, 64])\n",
      "blocks.0.attn.b_V torch.Size([16, 64])\n",
      "blocks.0.attn.W_O torch.Size([16, 64, 1024])\n",
      "blocks.0.attn.b_O torch.Size([16, 64])\n",
      "blocks.0.mlp.b_in torch.Size([4096])\n",
      "blocks.0.mlp.W_in torch.Size([1024, 4096])\n",
      "blocks.0.mlp.b_out torch.Size([1024])\n",
      "blocks.0.mlp.W_out torch.Size([4096, 1024])\n"
     ]
    }
   ],
   "source": [
    "for key, value in tl_dict.items():\n",
    "    if key.startswith(\"blocks.0.\"):\n",
    "        print(key, value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biogpt.layers.0.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "biogpt.layers.0.self_attn.k_proj.bias torch.Size([1024])\n",
      "biogpt.layers.0.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "biogpt.layers.0.self_attn.v_proj.bias torch.Size([1024])\n",
      "biogpt.layers.0.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "biogpt.layers.0.self_attn.q_proj.bias torch.Size([1024])\n",
      "biogpt.layers.0.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "biogpt.layers.0.self_attn.out_proj.bias torch.Size([1024])\n",
      "biogpt.layers.0.self_attn_layer_norm.weight torch.Size([1024])\n",
      "biogpt.layers.0.self_attn_layer_norm.bias torch.Size([1024])\n",
      "biogpt.layers.0.fc1.weight torch.Size([4096, 1024])\n",
      "biogpt.layers.0.fc1.bias torch.Size([4096])\n",
      "biogpt.layers.0.fc2.weight torch.Size([1024, 4096])\n",
      "biogpt.layers.0.fc2.bias torch.Size([1024])\n",
      "biogpt.layers.0.final_layer_norm.weight torch.Size([1024])\n",
      "biogpt.layers.0.final_layer_norm.bias torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "for name, param in state_dict.items():\n",
    "    if name.startswith(\"biogpt.layers.0.\"):\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.ln1.w torch.Size([1024])\n",
      "blocks.0.ln1.b torch.Size([1024])\n",
      "blocks.0.ln2.w torch.Size([1024])\n",
      "blocks.0.ln2.b torch.Size([1024])\n",
      "blocks.0.attn.W_Q torch.Size([16, 1024, 64])\n",
      "blocks.0.attn.W_O torch.Size([16, 64, 1024])\n",
      "blocks.0.attn.b_Q torch.Size([16, 64])\n",
      "blocks.0.attn.b_O torch.Size([1024])\n",
      "blocks.0.attn.W_K torch.Size([16, 1024, 64])\n",
      "blocks.0.attn.W_V torch.Size([16, 1024, 64])\n",
      "blocks.0.attn.b_K torch.Size([16, 64])\n",
      "blocks.0.attn.b_V torch.Size([16, 64])\n",
      "blocks.0.mlp.W_in torch.Size([1024, 4096])\n",
      "blocks.0.mlp.b_in torch.Size([4096])\n",
      "blocks.0.mlp.W_out torch.Size([4096, 1024])\n",
      "blocks.0.mlp.b_out torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"blocks.0.\"):\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
