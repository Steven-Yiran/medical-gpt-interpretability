{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading BioGPT into TransformerLens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import (\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x79bd57ac7910>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BioGptConfig {\n",
       "  \"_name_or_path\": \"microsoft/biogpt\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"architectures\": [\n",
       "    \"BioGptForCausalLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 1024,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 4096,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"layerdrop\": 0.0,\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"model_type\": \"biogpt\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"scale_embedding\": true,\n",
       "  \"transformers_version\": \"4.45.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 42384\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH = \"microsoft/biogpt\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
    "\n",
    "config = hf_model.config\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_path = \"BioGPT_state_dict.pth\"\n",
    "torch.save(hf_model.state_dict(), state_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load into TransformerLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_dim = config.hidden_size // config.num_attention_heads\n",
    "\n",
    "hooked_config = HookedTransformerConfig(\n",
    "    n_layers=config.num_hidden_layers,\n",
    "    d_model=config.hidden_size,\n",
    "    d_head=head_dim,\n",
    "    n_heads=config.num_attention_heads,\n",
    "    d_mlp=config.intermediate_size,\n",
    "    d_vocab=config.vocab_size,\n",
    "    n_ctx=config.max_position_embeddings,\n",
    "    act_fn=config.hidden_act,\n",
    ")\n",
    "model = HookedTransformer(hooked_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42384, 1024])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.W_E.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biogpt_to_transformer_lens_format(in_sd, n_layers, n_heads):\n",
    "    out_sd = {}\n",
    "    out_sd[\"pos_embed.W_pos\"] = in_sd[f\"biogpt.embed_positions.weight\"]\n",
    "    out_sd[\"embed.W_E\"] = in_sd[f\"biogpt.embed_tokens.weight\"]\n",
    "\n",
    "    out_sd[\"ln_final.w\"] = in_sd[f\"biogpt.layer_norm.weight\"]\n",
    "    out_sd[\"ln_final.b\"] = in_sd[f\"biogpt.layer_norm.bias\"]\n",
    "    out_sd[\"unembed.W_U\"] = in_sd[f\"output_projection.weight\"].T\n",
    "\n",
    "    for layer in range(n_layers):\n",
    "        out_sd[f\"blocks.{layer}.ln0.w\"] = in_sd[f\"biogpt.layers.{layer}.fc1.weight\"]\n",
    "        out_sd[f\"blocks.{layer}.ln0.b\"] = in_sd[f\"biogpt.layers.{layer}.fc1.bias\"]\n",
    "        out_sd[f\"blocks.{layer}.ln1.w\"] = in_sd[f\"biogpt.layers.{layer}.fc2.weight\"]\n",
    "        out_sd[f\"blocks.{layer}.ln1.b\"] = in_sd[f\"biogpt.layers.{layer}.fc2.bias\"]\n",
    "\n",
    "\n",
    "        out_sd[f\"blocks.{layer}.attn.W_Q\"] = einops.rearrange(\n",
    "            in_sd[f\"biogpt.layers.{layer}.self_attn.q_proj.weight\"],\n",
    "            \"(n_heads d_head) d_model -> n_heads d_model d_head\",\n",
    "            n_heads=n_heads,\n",
    "        )\n",
    "        out_sd[f\"blocks.{layer}.attn.b_Q\"] = einops.rearrange(\n",
    "            in_sd[f\"biogpt.layers.{layer}.self_attn.q_proj.bias\"],\n",
    "            \"(n_heads d_head) -> n_heads d_head\",\n",
    "            n_heads=n_heads,\n",
    "        )\n",
    "        out_sd[f\"blocks.{layer}.attn.W_K\"] = einops.rearrange(\n",
    "            in_sd[f\"biogpt.layers.{layer}.self_attn.k_proj.weight\"],\n",
    "            \"(n_heads d_head) d_model -> n_heads d_model d_head\",\n",
    "            n_heads=n_heads,\n",
    "        )\n",
    "        out_sd[f\"blocks.{layer}.attn.b_K\"] = einops.rearrange(\n",
    "            in_sd[f\"biogpt.layers.{layer}.self_attn.k_proj.bias\"],\n",
    "            \"(n_heads d_head) -> n_heads d_head\",\n",
    "            n_heads=n_heads,\n",
    "        )\n",
    "        out_sd[f\"blocks.{layer}.attn.W_V\"] = einops.rearrange(\n",
    "            in_sd[f\"biogpt.layers.{layer}.self_attn.v_proj.weight\"],\n",
    "            \"(n_heads d_head) d_model -> n_heads d_model d_head\",\n",
    "            n_heads=n_heads,\n",
    "        )\n",
    "        out_sd[f\"blocks.{layer}.attn.b_V\"] = einops.rearrange(\n",
    "            in_sd[f\"biogpt.layers.{layer}.self_attn.v_proj.bias\"],\n",
    "            \"(n_heads d_head) -> n_heads d_head\",\n",
    "            n_heads=n_heads,\n",
    "        )\n",
    "        out_sd[f\"blocks.{layer}.attn.W_O\"] = einops.rearrange(\n",
    "            in_sd[f\"biogpt.layers.{layer}.self_attn.out_proj.weight\"],\n",
    "            \"(d_model n_heads) d_head -> n_heads d_model d_head\",\n",
    "            n_heads=n_heads,\n",
    "        )\n",
    "        out_sd[f\"blocks.{layer}.attn.b_O\"] = einops.rearrange(\n",
    "            in_sd[f\"biogpt.layers.{layer}.self_attn.out_proj.bias\"],\n",
    "            \"(d_model n_heads) -> n_heads d_model\",\n",
    "            n_heads=n_heads,\n",
    "        )\n",
    "\n",
    "        out_sd[f\"blocks.{layer}.mlp.b_in\"] = in_sd[f\"biogpt.layers.{layer}.fc1.bias\"]\n",
    "        out_sd[f\"blocks.{layer}.mlp.W_in\"] = in_sd[f\"biogpt.layers.{layer}.fc1.weight\"].T\n",
    "        out_sd[f\"blocks.{layer}.mlp.b_out\"] = in_sd[f\"biogpt.layers.{layer}.fc2.bias\"]\n",
    "        out_sd[f\"blocks.{layer}.mlp.W_out\"] = in_sd[f\"biogpt.layers.{layer}.fc2.weight\"].T\n",
    "\n",
    "    return out_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (4096) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(state_dict_path, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m tl_dict \u001b[38;5;241m=\u001b[39m biogpt_to_transformer_lens_format(state_dict, config\u001b[38;5;241m.\u001b[39mnum_hidden_layers, config\u001b[38;5;241m.\u001b[39mnum_attention_heads)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_and_process_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtl_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/research-1/medical-gpt-interpretability/ENV/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:1558\u001b[0m, in \u001b[0;36mHookedTransformer.load_and_process_state_dict\u001b[0;34m(self, state_dict, fold_ln, center_writing_weights, center_unembed, fold_value_biases, refactor_factored_attn_matrices)\u001b[0m\n\u001b[1;32m   1554\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1555\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using MoE, so the layer norm weights can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be folded! Skipping\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1556\u001b[0m     )\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mnormalization_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLNPre\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m-> 1558\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfold_layer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mnormalization_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRMS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRMSPre\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1560\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfold_layer_norm(\n\u001b[1;32m   1561\u001b[0m         state_dict, fold_biases\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, center_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m     )\n",
      "File \u001b[0;32m~/research-1/medical-gpt-interpretability/ENV/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:1650\u001b[0m, in \u001b[0;36mHookedTransformer.fold_layer_norm\u001b[0;34m(self, state_dict, fold_biases, center_weights)\u001b[0m\n\u001b[1;32m   1639\u001b[0m     state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.attn.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgqa\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mb_V\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m state_dict[\n\u001b[1;32m   1640\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.attn.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgqa\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mb_V\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1641\u001b[0m     ] \u001b[38;5;241m+\u001b[39m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m   1646\u001b[0m     )\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ln1.b\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1649\u001b[0m state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.attn.W_Q\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1650\u001b[0m     \u001b[43mstate_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43ml\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.attn.W_Q\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43ml\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.ln1.w\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\n\u001b[1;32m   1651\u001b[0m )\n\u001b[1;32m   1652\u001b[0m state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.attn.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgqa\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mW_K\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1653\u001b[0m     state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.attn.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgqa\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mW_K\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;241m*\u001b[39m state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ln1.w\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;28;01mNone\u001b[39;00m, :, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m   1655\u001b[0m )\n\u001b[1;32m   1656\u001b[0m state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.attn.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgqa\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mW_V\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1657\u001b[0m     state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.attn.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgqa\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mW_V\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1658\u001b[0m     \u001b[38;5;241m*\u001b[39m state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ln1.w\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;28;01mNone\u001b[39;00m, :, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m   1659\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (4096) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load(state_dict_path, weights_only=False)\n",
    "\n",
    "tl_dict = biogpt_to_transformer_lens_format(state_dict, config.num_hidden_layers, config.num_attention_heads)\n",
    "model.load_and_process_state_dict(tl_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
